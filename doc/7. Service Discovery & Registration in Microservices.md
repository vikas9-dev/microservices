# Service Discovery & Registration in Microservices

## 1. Understanding Internal Communication in Microservices

Let us explore one of the critical challenges that arise when building microservices applications: **internal communication** between microservices.

### A Quick Recap of Our Microservices Setup

So far, we have built three different microservices:

- **Accounts Microservice**
- **Loans Microservice**
- **Cards Microservice**

Each of these microservices is responsible for handling specific business logic, storing data, and processing incoming requests. Whenever an application interacts with these microservices, they execute the necessary logic and return an appropriate response.

### Deployment of Microservices in a Network

In real-world applications, microservices are not deployed in isolation on separate servers. Instead, they are hosted within a **common microservices network**. This allows them to communicate efficiently while remaining secure.

To regulate access to our microservices, we introduce a **single entry point**—the **API Gateway**. External clients (such as C1, C2, and C3) cannot communicate directly with the microservices. Instead, all incoming requests must pass through this API Gateway, which acts as a filter to enforce security, auditing, and logging. This ensures that only authorized traffic reaches the microservices.

### The Importance of Internal Communication

While the API Gateway handles **external traffic**, microservices often need to interact with each other internally. For example:

- An external request may arrive at the **Accounts Microservice**.
- To process the request, Accounts may need to fetch additional details from the **Loans** and **Cards** microservices.
- These interactions between microservices are referred to as **internal communication**.

In a microservices network, we classify traffic into two types:

- **External Communication:** Traffic entering from outside, passing through the API Gateway.
- **Internal Communication:** Traffic flowing between microservices within the network.

### Challenges in Internal Communication

Internal communication is not as straightforward as external API calls. It introduces several challenges, including:

- **Service Discovery:** How does one microservice locate another within the network?
- **Load Balancing:** Ensuring requests are distributed efficiently among service instances.
- **Fault Tolerance:** Handling failures when one microservice depends on another.
- **Latency & Performance:** Minimizing delays in inter-service communication.
- **Security & Authorization:** Ensuring that microservices interact securely without exposing vulnerabilities.

---

## 2. The Challenge of Service Discovery in Microservices

In this segment, we will explore another crucial challenge in building microservices: **service discovery and registration**.

So far, we have discussed four different challenges in microservices and how to overcome them. Now, we move on to **Challenge #5**:

- **How do microservices discover and register themselves in a microservices network?**

Before diving into solutions, let’s first understand the problem in detail.

### The Problem: How Do Microservices Locate Each Other?

All our microservices are deployed within a **microservices network**. When one microservice instance needs to communicate with another, it must locate it within this network. A common assumption is that each microservice has a fixed **endpoint (IP address + port number)** that other services can use to connect. While this might work in monolithic applications, microservices architecture presents additional challenges:

- Microservice containers are created and destroyed dynamically.
- Scaling up or down changes the number of service instances.
- Each time a microservice restarts, it may receive a **new IP address**.

Given these dynamic conditions, how can one microservice reliably find another? This brings us to the challenge of **service discovery**.

### The Problem: How Do New Instances Register Themselves?

Imagine a production environment where we initially deploy:

- 1 instance of the **Accounts Microservice**
- 1 instance of the **Loans Microservice**
- 1 instance of the **Cards Microservice**

Later, as traffic increases, we scale up to **5 instances of each service**. When new instances are added, they must:

- Register themselves in the network.
- Inform other microservices about their presence.

Without proper registration, the **Accounts Microservice** might think that only one instance of the **Loans Microservice** exists, while in reality, five instances are running. This lack of awareness can lead to inefficient communication and bottlenecks.

Similarly, when replacing an unhealthy instance with a new one, the new container will have a different **IP address**. If the system isn’t aware of these changes, it won’t be able to route traffic correctly.

### The Problem: Load Balancing Between Multiple Instances

Another challenge arises when multiple instances of a microservice exist. Suppose the **Accounts Microservice** needs to call the **Loans Microservice**, which has five running instances. The key questions here are:

- How does the **Accounts Microservice** decide which instance of the **Loans Microservice** to call?
- How do we ensure traffic is distributed evenly among all instances?

Without proper load balancing, one instance might receive all the traffic, becoming overloaded, while others remain idle. This results in poor performance and inefficient resource utilization.

### Introducing the Solution: Service Discovery, Registration, and Load Balancing

To tackle these challenges, we use established patterns and concepts:

1. **Service Discovery** – A mechanism that enables microservices to locate each other dynamically.
2. **Service Registration** – Ensures that new instances register themselves so they can be discovered by others.
3. **Load Balancing** – Distributes traffic evenly among multiple instances to prevent overload.

---

## 3. Challenges of Traditional Approaches in Microservices Communication

For a moment, let’s imagine a microservices environment without service discovery or service registration. In this scenario, we are forced to rely on traditional monolithic approaches for internal communication between services. This presents several challenges that can significantly impact the efficiency, scalability, and maintainability of our system.

### Direct Service Communication: The Traditional Way

In a typical web network, a service must know the exact location of another service to communicate with it. This is usually done using:

- An **IP address** (e.g., `127.54.37.23`)
- A **DNS name** or **domain name** as an abstraction over the IP address

Now, let’s consider a scenario where the **Accounts** microservice needs to communicate with the **Loans** microservice. For simplicity, assume there is only one instance of each service running.

- The **Accounts** microservice is deployed in a server, making it the **upstream service** because it depends on another service.
- The **Loans** microservice is the **downstream service**, acting as a dependency for Accounts.

To establish communication, the Accounts microservice has two options:

1. **Hardcoding the IP address** of the Loans service within its code.
2. **Using a DNS name**, which is mapped to the Loans service's IP address.

This works fine in a static environment where the Loans service always has the same IP address. However, in cloud environments, where services dynamically scale up or down, this approach introduces critical limitations.

### Why Traditional Communication Fails in Microservices

In a monolithic or SOA-based system, the number of services is limited, and their deployments are relatively stable. Maintaining a DNS-to-IP mapping is manageable. However, in a **microservices-based architecture**, where services are containerized and frequently redeployed, this approach becomes impractical due to:

#### 1. **Dynamic IP Changes and Scaling Issues**

- Microservices architectures involve **dynamic scaling** based on traffic demand.
- When new instances of a service are created or old ones are removed, their IP addresses change.
- **Hardcoded IPs become obsolete**, requiring constant updates to maintain service connectivity.

#### 2. **Load Balancing Limitations**

- Traditional load balancers require **static routing tables**, which do not adapt well to **frequent service scaling**.
- If a load balancer is not updated with new service instances, some requests may fail.
- Load balancers in traditional setups also introduce additional costs and potential **single points of failure**.

#### 3. **Manual Maintenance Complexity**

- Someone must **manually update DNS records** or routing tables whenever a microservice instance is created or removed.
- This is **not feasible** in a cloud-native environment where services scale dynamically and are often ephemeral (short-lived).

#### 4. **Inefficient Failover and High Latency**

- If a microservice instance goes down, the system must detect and remove it from the routing table.
- Traditional DNS resolution **does not provide real-time updates**, leading to delayed failover responses.

### The Need for a Better Approach

To overcome these limitations, cloud-native microservices rely on **service discovery mechanisms** and **dynamic load balancing** solutions, which automatically track service instances and distribute requests efficiently.

In the next section, we will explore how modern solutions like **Eureka, Consul, and Kubernetes Service Discovery** address these challenges, enabling seamless microservices communication without relying on static configurations.

---

## 4. Solving Microservices Communication Challenges with Service Discovery

In the previous section, we discussed the limitations of traditional load balancers in microservices and cloud-native applications. Now, let's explore how we can overcome these challenges using the **Service Discovery pattern**.

### What is Service Discovery?

Service Discovery is a pattern that helps microservices **automatically track, register, and locate** each other without relying on hardcoded IP addresses or static configurations. This is achieved through a **Service Registry**, which acts as a centralized database that maintains a list of all active service instances.

Whenever a new microservice instance starts, it **registers itself** with the service registry. Similarly, when a microservice instance is shut down or removed, it is **deregistered automatically**. This ensures that only active and healthy instances are discoverable by other services.

For example, if five instances of the **Loans** microservice are running, they all register with the service registry. The **Accounts** microservice can then query the registry to dynamically discover an available Loans service instance instead of relying on a hardcoded IP address.

### How Does Service Discovery Work?

Service Discovery consists of two main processes:

1. **Service Registration** – Each microservice instance registers its details (such as IP address and port) with the **Service Registry** upon startup.
2. **Service Lookup (Discovery)** – When another microservice needs to communicate, it queries the **Service Registry** to find the current, healthy instances of the target service.

This eliminates the need for manual updates whenever services scale up or down.

### Service Discovery and Load Balancing

Apart from helping with service registration, the **Service Registry also facilitates load balancing**. If multiple instances of a service are available, the registry ensures requests are distributed among them using a load-balancing strategy.

There are two approaches to Service Discovery:

#### 1. **Client-Side Service Discovery**

- The client (requesting microservice) queries the **Service Registry** directly to find available service instances.
- The client then selects an instance and communicates with it directly.
- **Example:** Netflix Eureka follows this approach.

#### 2. **Server-Side Service Discovery**

- The client sends a request to a **load balancer** (such as an API gateway or service mesh).
- The load balancer queries the **Service Registry** and routes the request to an available instance.
- **Example:** Kubernetes Service Discovery follows this model.

In this section, we will focus on **Client-Side Service Discovery**, while later in the course, we will explore **Server-Side Discovery** using Kubernetes.

### Components of Service Discovery

1. **Service Registry (Central Server):**

   - Maintains a list of all active microservices.
   - Similar to a **Config Server** but for service addresses instead of configurations.

2. **Service Registration Process:**

   - When a microservice starts, it registers its IP and port with the **Service Registry**.
   - It also sends periodic **heartbeat signals** to confirm its availability.

3. **Service Deregistration:**
   - If a microservice fails or shuts down, the **Service Registry** removes its details.
   - If no heartbeat signals are received, the registry assumes the service is unhealthy and removes it automatically.

### Why Service Discovery is Essential

- **Eliminates Hardcoded IP Addresses:** Services dynamically discover each other.
- **Supports Auto-Scaling:** As instances increase or decrease, the registry keeps track.
- **Improves Reliability:** Unhealthy services are automatically removed.
- **Enhances Load Balancing:** Requests are intelligently distributed among healthy instances.

### Conclusion

Service Discovery and Service Registration solve the fundamental problem of **how microservices communicate in a dynamic environment**. By implementing a **centralized service registry**, we ensure seamless interaction between services, even as they scale up or down.

In the next section, we will dive deeper into **Client-Side Service Discovery** and see it in action.

---

## 5. Client-Side Service Discovery in Microservices

Previously we explored the concept of service discovery and service registration, which help solve communication challenges in microservices. We also introduced two approaches to implementing service discovery:

1. **Client-Side Service Discovery**
2. **Server-Side Service Discovery**

Let's focus on **Client-Side Service Discovery**, explaining how it works and how it can be implemented in microservices.

---

### What is Client-Side Service Discovery?

Client-Side Service Discovery is an approach where microservices are responsible for registering themselves with a **Service Registry** upon startup and deregistering when they shut down. This ensures that an updated list of available services is maintained dynamically.

When a microservice needs to communicate with another service, it queries the **Service Registry** to obtain the available service instances. If multiple instances exist, the requesting microservice selects one using a **load balancing strategy**.

---

### How Client-Side Service Discovery Works

1. **Service Registration**

   - Before launching microservices, the **Service Registry** must be running.
   - Each microservice instance registers itself with the registry upon startup, providing details such as **IP address, hostname, and port number**.
   - The service periodically sends **heartbeat signals** to confirm its availability.

2. **Service Discovery**

   - When a microservice (e.g., `Accounts` service) wants to communicate with another (`Loans` service), it queries the **Service Registry** for available instances.
   - The **Service Registry** responds with a list of available instances of the `Loans` service.

3. **Client-Side Load Balancing**

   - The requesting microservice (`Accounts` service) is responsible for selecting one instance from the list.
   - It applies a **load balancing strategy** such as:
     - **Round Robin**: Requests are distributed sequentially.
     - **Weighted Round Robin**: Requests are distributed based on assigned weights.
     - **Least Connections**: The instance with the fewest connections is selected.
     - **Custom Algorithms**: Custom strategies can also be implemented.

4. **Caching Service Details**
   - To optimize performance, the requesting microservice caches the discovered service instances.
   - Cached details reduce dependency on the Service Registry for each request.
   - The cache is refreshed periodically (e.g., every 10-20 seconds) or when an error occurs, triggering an update.

---

### Advantages of Client-Side Service Discovery

- **Flexibility in Load Balancing:** Different strategies can be applied to optimize traffic distribution.
- **Reduced Dependency on External Load Balancers:** Eliminates the need for traditional load balancers.
- **Improved Performance:** Caching reduces the load on the Service Registry and speeds up service lookups.

---

### Challenges of Client-Side Service Discovery

- **Increased Complexity for Developers:** Each microservice must include logic for service registration, discovery, and load balancing.
- **Need for a Centralized Service Registry:** A separate server must be maintained for service registration and discovery.

---

### Comparison with Server-Side Service Discovery

- **Client-Side Discovery** is suitable for projects with limited budgets that cannot afford Kubernetes clusters.
- **Server-Side Discovery** is preferred for Kubernetes-based deployments, as Kubernetes handles service registration and discovery automatically.

### Simplifying Client-Side Service Discovery with Spring Cloud

While implementing client-side service discovery from scratch may seem complex, **Spring Cloud** simplifies this process. Spring Cloud provides built-in solutions for service registration, discovery, and load balancing, which we will explore in upcoming discussions.

---

## 6. **Client-Side Service Discovery in Microservices**

Spring Cloud provides a simple and efficient way to achieve service discovery and registration. Let us explore its implementation and realize how seamless the process can be.

### Key Components of Client-Side Service Discovery

To achieve service discovery and load balancing, we will leverage several Spring Cloud components:

#### **1. Eureka Server (Spring Cloud Netflix Eureka)**

- Acts as a central service responsible for service registration and discovery.
- Functions as a service discovery agent in the microservices architecture.

#### **2. Spring Cloud Load Balancer**

- Enables client-side load balancing.
- Replaces Netflix Ribbon, which is now in maintenance mode and no longer actively developed.

#### **3. Netflix Feign Client**

- Provides a simple and declarative way to make REST API calls between microservices.
- Functions similarly to `RestTemplate` and `WebClient` in the Spring ecosystem.

### Alternative Technologies

While we are using Spring Cloud components, there are other service discovery tools available in the industry:

- **Etcd**
- **Consul**
- **Apache Zookeeper**

Since we are building microservices with Spring Boot, using Spring Cloud components ensures seamless integration and ease of implementation.

### Evolution of Client-Side Load Balancing

In older projects using earlier versions of Spring Boot, Netflix Ribbon was commonly used. However, it is now deprecated in favor of **Spring Cloud Load Balancer**, which is the preferred and actively maintained solution.

### Advantages of Client-Side Service Discovery

- **High Availability:** Multiple nodes of service discovery can be deployed, reducing the risk of downtime.
- **Dynamic Configuration Updates:** IP addresses and load balancer configurations update dynamically without affecting microservices communication.
- **Fault Tolerance and Resilience:** Service discovery mechanisms enhance reliability by providing failover and recovery mechanisms.

### The Role of Netflix in Spring Cloud

Spring Cloud Netflix originated from libraries developed by Netflix, such as:

- **Eureka (Service Discovery)**
- **Ribbon (Load Balancing - now replaced by Spring Cloud Load Balancer)**
- **Hystrix (Fault Tolerance - now replaced by Resilience4j)**

Netflix open-sourced these components in 2012, and they were integrated into Spring Cloud in 2015. Since then, the project has evolved, and Netflix itself has adopted Spring Boot and Spring Cloud Netflix for its microservices architecture.

### Conclusion

We are using modern, stable, and widely adopted technologies in our course. Netflix's adoption of these technologies is a testament to their reliability and scalability. Further reading:- [Netflix OSS and Spring Boot](https://netflixtechblog.com/netflix-oss-and-spring-boot-coming-full-circle-4855947713a0)

---

## 7. Setting Up a Service Discovery Agent with Eureka

Let us build a **service discovery agent** using **Eureka** from the **Spring Cloud Netflix** project.

### Project Setup

Since this is a new section, we create a new folder **section 8** inside the workspace. Instead of copying code from **section 7** (which uses MySQL), we copy the code from **section 6 v2**, as we have decided to use **H2 database**.

After setting up the folder, we open it in **IntelliJ IDEA**, remove unnecessary dependencies like **Spring Cloud Bus** and **RabbitMQ** from all microservices (`accounts`, `loans`, `cards`), and clean up the **application.yml** files accordingly.

### Creating the Eureka Server

To create the Eureka Server:

1. Visit [start.spring.io](https://start.spring.io/).
2. Use **Maven** as the build tool and **Java** as the language.
3. Select the latest stable **Spring Boot** version.
4. Set:
   - **Group**: `com.knowprogram`
   - **Artifact & Name**: `eurekaserver`
   - **Description**: `Service Discovery Agent for PeopleBank Microservices`
   - **Java Version**: 17
5. Add dependencies:
   - `Eureka Server`
   - `Config Client`
   - `Spring Boot Actuator`
6. Generate the project, extract the ZIP, and move it into `section 8`.

### Loading the Eureka Server

In **IntelliJ IDEA**:

- Add the **Eureka Server** as a Maven project.
- Open the **main class** and add `@EnableEurekaServer`.
- Allow **Maven dependencies** to download.

### Configuring the Eureka Server

#### 1. Update `application.yml`

- Set the **application name**:
  ```yaml
  spring:
    application:
      name: eureka-server
  ```
- Connect to the **Config Server**:
  ```yaml
  spring:
    config:
      import: "optional:configserver:http://localhost:8071"
  ```
- Enable health checks for **Docker Compose**:
  ```yaml
  management:
    endpoints:
        web:
        exposure:
            include: "*"
    health:
        readiness-state:
        enabled: true
        liveness-state:
        enabled: true
    endpoint:
        health:
        probes:
            enabled: true
  ```

#### 2. Add Configuration in GitHub Repository

Inside the `peoplebank-config` repository, create a file **eurekaserver.yml**:

```yaml
server:
  port: 8070

eureka:
  instance:
    hostname: localhost
  client:
    fetch-registry: false
    register-with-eureka: false
    service-url:
      defaultZone: "http://${eureka.instance.hostname}:${server.port}/eureka"
```

### Running the Eureka Server

1. **Start the Config Server** (`localhost:8071`).
2. Verify Eureka properties using:
   ```
   localhost:8071/eurekaserver/default
   ```
3. Start the **Eureka Server** in debug mode.
4. Verify Eureka at (in Browser): [http://localhost:8070](localhost:8070)

At this stage, the Eureka dashboard will be empty since no microservices have registered yet. The next step is to integrate our microservices (`accounts`, `loans`, `cards`) with Eureka.

---

## 8. Registering Microservices with Eureka

Let us see how to connect the Accounts Microservice to Eureka.

### Step 1: Add the Eureka Client Dependency  

To enable service registration, we need to add the **Eureka Client** dependency to our `accounts` microservice.  

   ```xml
  <dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
  </dependency>
   ```

### Step 2: Configuring `application.yml`  

Now, we need to modify the `application.yml` file of the **accounts** microservice to register it with Eureka.  

#### Eureka Configuration  

```yaml
eureka:
  instance:
    preferIpAddress: true
  client:
    fetchRegistry: true
    registerWithEureka: true
    serviceUrl:
      defaultZone: http://localhost:8070/eureka/
```

- `fetchRegistry: true` → Ensures the microservice fetches the **registry details** to communicate with other services. The default vale of this property is `true`, therefore it is optional.
- `registerWithEureka: true` → Allows the microservice to **register itself** with Eureka. The default vale of this property is `true`, therefore it is optional.
- `defaultZone` → Defines the **Eureka server URL** (must match the one in `eurekaserver` project).  
- `preferIpAddress: true` → Ensures the service registers using its **IP address** instead of hostname.  

#### Actuator & Info Configuration  

We also add **actuator** properties to expose service information:  

```yaml
info:
  app:
    name: accounts
    description: PeopleBank Accounts Application
    version: 1.0.0

management:
  endpoints:
    web:
      exposure:
        include: "*"
  endpoint:
    health:
      show-details: always
  info:
    env:
      enabled: true
```

- `info.app` → Displays **service details** in the **Eureka dashboard**.  
- `management.endpoints.web.exposure.include: "*"` → Exposes all **actuator endpoints**.  
- `management.info.env.enabled: true` → Enables **environment information**.  

#### Enabling Shutdown Endpoint  

To allow graceful shutdown via Actuator, we enable the **shutdown endpoint**:  

```yaml
management:
  endpoints:
    web:
      exposure:
        include: "*"
  endpoint:
    # add shutdown endpoint
    shutdown:
      enabled: true

# define it at the root level also
endpoints:
    shutdown:
      enabled: true
```

- This enables `/actuator/shutdown` for **graceful service termination**.  

### Step 3: Running the Accounts Microservice  

1. **Start the Config Server** (`localhost:8071`).  
2. **Start the Eureka Server** (`localhost:8070`).  
3. **Start the Accounts Microservice** (`localhost:8080`).  

Upon startup, the service should **register** with Eureka and send **heartbeat signals** every **30 seconds**. You will see a **204 registration response** in the logs.  

### Step 4: Verifying in Eureka Dashboard  

1. Open the Eureka Dashboard:  [http://localhost:8070](localhost:8070)
2. You should see `accounts` listed under **Instances Registered with Eureka**.  
3. Clicking on `accounts` will show details like:  
   - **App Name:** accounts  
   - **Description:** PeopleBank Accounts Application  
   - **Version:** 1.0.0  
   - **IP Address Registration**  

Now that `accounts` is registered, please repeat these steps for:  
- **loans** microservice  
- **cards** microservice  

On the Eureka Dashboard: [http://localhost:8070/eureka/apps](localhost:8070/eureka/apps), you should see `accounts`, `loans` and `cards` listed as **Registered Applications**. Through this URL, one can get the registered microservices details from Eureka server. By default it gives XML response. To get the JSON response, in Postman, use Header `Accept: application/json`. To get Accounts microservice specific details only, use the following URL: [http://localhost:8070/eureka/apps/accounts](localhost:8070/eureka/apps/accounts).

---

## 9. Graceful Shutdown and Eureka Deregistration

Previously, we saw how each of our microservices successfully registers with the Eureka Server during startup. Let us explore how our microservices deregister themselves from the Eureka Server during a graceful shutdown.

**Why Graceful Shutdown Matters**

When working in production-like environments such as `dev`, `qa`, or `prod`, it's important to avoid terminating services abruptly—for instance, by stopping them directly via IntelliJ IDEA. Doing so skips crucial shutdown logic, such as deregistering from the service registry.

Instead, we follow a controlled shutdown process using actuator endpoints or scripts, ensuring the service performs any final tasks—like deregistration—before completely stopping.

### Enabling Shutdown Endpoints

We've already configured shutdown endpoints in our `accounts`, `cards`, and `loans` microservices by setting relevant properties under the `management` section. This exposes a shutdown URL via Spring Boot Actuator, allowing us to shut down services gracefully.

```yaml
management:
  endpoints:
    web:
      exposure:
        include: "*"
  endpoint:
    shutdown:
      enabled: true

# define it at the root level also
endpoints:
    shutdown:
      enabled: true
```

For example, in the `accounts` microservice, the shutdown URL is:

```
http://localhost:8080/actuator/shutdown
```

> Note: This endpoint only supports HTTP POST requests.


```
{"message":"Shutting down, bye..."}
```

Now, if you check the Eureka dashboard and refresh the page, you’ll notice the `AccountsApplication` is no longer listed. That’s because it has successfully deregistered from the Eureka Server. In the Intellij also, accounts microservice is stopped.

Additionally, if you check the logs of the `accounts` service, you’ll find messages indicating the service is stopping, deregistering, and that a status `200 OK` was returned by Eureka—confirming a successful deregistration.

You can repeated the same steps for the `cards` {[http://localhost:9000/actuator/shutdown](http://localhost:9000/actuator/shutdown)} and `loans` services:

Once all services are gracefully shut down, the Eureka dashboard shows no registered instances—demonstrating that service discovery is inactive when no services are available.

In the next section, we’ll explore **heartbeat behavior**—how microservices send regular updates to Eureka to indicate that they're alive and healthy.

---

## 10. Microservice Heartbeats: Staying Alive in Eureka

After registering with the Eureka Server, microservices don’t just sit idle—they send **heartbeat signals every 30 seconds** to confirm they’re healthy. This keeps Eureka’s service registry accurate and up-to-date.

1. **Start all microservices** (`loans`, `cards`, and `accounts`) and verify they appear on the Eureka dashboard.
2. **Clean the logs**, then **shut down the Eureka Server**.
3. After ~30 seconds, you'll see exceptions in the microservices' logs indicating they were **unable to send heartbeat**.
4. Logs show failed `PUT` requests to Eureka—confirming services are still trying to communicate.

```
Request execution failed with message: I/O error on PUT request for "http://localhost:8070/eureka/apps/ACCOUNTS/192.168.1.9:accounts:8080": Connect to http://localhost:8070 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused: no further information

DiscoveryClient_ACCOUNTS/192.168.1.9:accounts:8080 - was unable to send heartbeat!

com.netflix.discovery.shared.transport.TransportException: Cannot execute request on any known server
```

**Key Takeaways**

- Microservices **register automatically** at startup.
- They **send heartbeats** every 30 seconds.
- If Eureka is down, they log errors but continue trying.
- **All communication is automatic**—no manual steps needed.

---

## 11. 🔍 Service Discovery & Client-Side Load Balancing with Eureka and OpenFeign

Now that our microservices are registered with Eureka, let's explore how they **discover and communicate** with one another. We'll also see how **client-side load balancing** is handled automatically when using Eureka with **OpenFeign**.

### Building a Unified Customer Details API

To demonstrate this, we’ll create a new REST API in the `accounts` microservice. This API will gather customer data from three services:

- **Accounts** (local data)
- **Loans** (via `loans` service)
- **Cards** (via `cards` service)

Since `accounts` doesn’t store loan or card details, it needs to fetch them by **internally calling other microservices**.

### Setting Up OpenFeign

To simplify inter-service communication:
1. Add the **OpenFeign dependency** in `pom.xml`.

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

2. Annotate the `AccountsApplication` class with `@EnableFeignClients`.
3. Create Feign client interfaces like `CardsFeignClient` and `LoansFeignClient` using `@FeignClient("cards")` and `@FeignClient("loans")`.

These interfaces should:
- Match the target service's REST API signatures.
- Include request mappings like `/api/fetch`.
- Use shared DTOs (like `CardsDto`, `LoansDto`) copied from their respective services.

```java
@FeignClient("cards") // microservice name
public interface CardsFeignClient {
    @GetMapping(value = "/api/fetch", consumes = "application/json")
    public ResponseEntity<CardsDto> fetchCardDetails(@RequestParam String mobileNumber);
}
```

### Why OpenFeign?

Unlike `RestTemplate` or `WebClient`, OpenFeign lets you **skip writing HTTP boilerplate**. Just define **interfaces and method signatures**—Feign handles the rest, including:
- **Discovering service instances** via Eureka
- **Load balancing** across available instances
- **Serializing/deserializing** JSON payloads
- **Retrying on failure**

### Behind the Scenes

Once a Feign client is invoked:
- It queries Eureka for service instances (e.g., for `cards`).
- Caches instance info for 30 seconds (by default).
- Picks one instance (client-side load balancing) and invokes the API.

You get seamless, declarative service-to-service communication with minimal effort.

---

## 12. Consolidating Customer Data Across Microservices

With the Feign clients now set up in our `accounts` microservice (`LoansFeignClient` and `CardsFeignClient`), the next step is to use these clients to fetch data from the `loans` and `cards` microservices. The goal is to aggregate the customer's account, loan, and card information, along with their personal details, and return it to the client application in a single response.

### Creating a DTO for Consolidated Data

To facilitate this, we need a new DTO that can encapsulate all customer-related information. We'll name this DTO `CustomerAllDetailsDto`. This class will hold the customer's personal details, account information, loans, and cards data.

```java
@Data
public class CustomerAllDetailsDto {
  private String name;
  private String email;
  private String mobileNumber;
  private AccountDTO accountDTO;
  private LoansDto loansDto;
  private CardsDto cardsDto;
}
```

### Building the Controller

We introduce a new controller, `CustomerController`, with a REST endpoint `/api/fetchCustomerDetails`. This endpoint accepts a customer’s mobile number and returns their full details. 

The method `fetchCustomerDetails()` performs the following steps:

1. **Validates** the mobile number format.
2. **Delegates** the business logic to the service layer (`ICustomerService`).
3. **Returns** the consolidated `CustomerAllDetailsDto` wrapped in a `ResponseEntity`.

```java
@RestController
@RequestMapping("/api")
@Validated
@AllArgsConstructor
public class CustomerController {

    private ICustomerService customerService;

    @GetMapping("/fetchCustomerDetails")
    public ResponseEntity<CustomerAllDetailsDto> fetchCustomerDetails(@RequestParam @Pattern(regexp = "(^$|[0-9]{10})", message
            = "Mobile Number must be 10 digits") String mobileNumber) {
        CustomerAllDetailsDto customerAllDetailsDto = customerService.fetchCustomerDetails(mobileNumber);
        return ResponseEntity.status(HttpStatus.OK).body(customerAllDetailsDto);
    }

}
```

### Implementing the Service Layer

In the service layer, we define the `ICustomerService` interface with a method `fetchCustomerDetails(String mobileNumber)` and implement it in `CustomerServiceImpl`.

**Dependencies Injected:**

- `AccountsRepository` and `CustomerRepository` (for local DB access)
- `LoansFeignClient` and `CardsFeignClient` (for remote service calls)

Using Lombok’s `@AllArgsConstructor`, these dependencies are injected without boilerplate constructors.

**Writing Business Logic**

Inside `fetchCustomerDetails()`, we follow these steps:

1. **Fetch customer and account details** from the database using the repositories.
2. **Map the data** to `CustomerAllDetailsDto` using a new method `mapToCustomerAllDetailsDto()` in `CustomerMapper`.
3. **Invoke Feign clients** to call `fetchLoanDetails()` and `fetchCardDetails()` using the mobile number.
4. **Extract responses** using `getBody()` and populate them into `CustomerAllDetailsDto`.
5. **Return** the fully populated DTO.

```java
@Service
@AllArgsConstructor
public class CustomerServiceImpl implements ICustomerService {

    private AccountRepository accountRepository;
    private CustomerRepository customerRepository;
    private LoansFeignClient loansFeignClient;
    private CardsFeignClient cardsFeignClient;

    @Override
    public CustomerAllDetailsDto fetchCustomerDetails(String mobileNumber) {
        Customer customer =
                customerRepository.findByMobileNumber(mobileNumber).orElseThrow(() -> new ResourceNotFoundException(
                        "Customer", "mobileNumber", mobileNumber));
        Account account =
                accountRepository.findByCustomerId(customer.getCustomerId()).orElseThrow(() -> new ResourceNotFoundException("Account", "customerId", String.valueOf(customer.getCustomerId())));
        CustomerAllDetailsDto customerAllDetailsDto = CustomerMapper.mapToCustomerAllDetailsDto(customer,
                new CustomerAllDetailsDto());
        customerAllDetailsDto.setAccountDTO(AccountMapper.mapToAccountDTO(account, new AccountDTO()));

        ResponseEntity<LoansDto> loansDtoResponseEntity = loansFeignClient.fetchLoanDetails(mobileNumber);
        if (loansDtoResponseEntity.getBody() != null)
            customerAllDetailsDto.setLoansDto(loansDtoResponseEntity.getBody());

        ResponseEntity<CardsDto> cardsDtoResponseEntity = cardsFeignClient.fetchCardDetails(mobileNumber);
        if (cardsDtoResponseEntity.getBody() != null)
            customerAllDetailsDto.setCardsDto(cardsDtoResponseEntity.getBody());

        return customerAllDetailsDto;
    }
}
```

### Testing the Integration

Once the logic is complete:

1. Start all microservices and ensure they are registered with the Eureka Server.
2. Use Postman to create data in `accounts`, `loans`, and `cards` services with the same mobile number.
3. Invoke the `fetchCustomerDetails` API from `accounts` service.

The response should now include all personal, account, loans, and card details.

### Eureka + Feign = Seamless Communication

Thanks to Eureka and Feign, the `accounts` service doesn't need hardcoded URLs for other services. It simply uses logical service names, and Eureka resolves the correct instances. Feign, with the help of Spring Cloud LoadBalancer, handles all the networking and load balancing transparently.

This seamless integration ensures our microservices communicate effectively with minimal configuration, empowering developers to focus on business logic rather than infrastructure concerns.

---

## 13. Understanding Eureka Self-Preservation Mode

If you've ever noticed a warning on the Eureka Dashboard that reads something like:

> **"EMERGENCY: Eureka may be incorrectly claiming instances are up when they are not. Renewals are lesser than threshold and hence the instances are not being expired just to be safe."**

…it can be a bit confusing at first. But this message starts to make perfect sense once you understand an important built-in mechanism of the Eureka Server: **Self-Preservation Mode**.

### What Is Self-Preservation Mode?

In a typical microservices setup, the Eureka Server expects **heartbeat signals** (renewals) from all registered service instances at regular intervals. These heartbeats help Eureka determine whether a service instance is healthy and available.

Normally, if Eureka doesn’t receive a heartbeat from a particular instance within a defined time (90 seconds by default), it considers that instance unresponsive or unhealthy and removes it from the registry. This helps ensure that only healthy services are discoverable.

However, what if the heartbeat failures are caused not by actual service crashes, but by temporary **network glitches**?

Let’s consider a real-world scenario:  
If there’s a brief 5-minute network outage between Eureka and your microservices, Eureka will miss several heartbeats from **all** instances. If it reacts immediately by evicting them from the registry, the service registry could become empty—leading to widespread instability across your system.

### Enter Self-Preservation Mode

To prevent such false alarms, Eureka has a safeguard called **self-preservation mode**.

When the Eureka Server detects that a **large percentage of heartbeats** are missing (typically, when renewals fall below 85% of the expected rate), it enters this special mode. While in self-preservation:

- **Eureka stops evicting instances** from the registry—even if heartbeats are missing.
- It assumes that the issue might be temporary (like a network hiccup).
- It continues serving whatever instance information it already has to the client microservices.
  
This approach helps **maintain stability and availability** during brief outages or infrastructure blips.

Once the network stabilizes and Eureka starts receiving heartbeats again above the threshold, it exits self-preservation mode and resumes normal eviction behavior.

### How It Works Internally

Here are the key Eureka properties related to heartbeats and self-preservation:

| Property | Default | Purpose |
|---------|---------|---------|
| `eureka.instance.lease-renewal-interval-in-seconds` | 30s | Time interval between client heartbeats |
| `eureka.instance.lease-expiration-duration-in-seconds` | 90s | Time Eureka waits before considering an instance unhealthy |
| `eureka.server.eviction-interval-timer-in-ms` | 60,000 (60s) | Frequency at which Eureka performs eviction |
| `eureka.server.renewal-percent-threshold` | 0.85 (85%) | Minimum percentage of expected renewals before entering self-preservation |
| `eureka.server.renewal-threshold-update-interval-ms` | 900,000 (15 min) | How often Eureka recalculates expected renewal thresholds |
| `eureka.server.enable-self-preservation` | true | Enables or disables self-preservation mode |

### Visual Scenario

Imagine you have five instances of the `loans` service registered with Eureka. Normally, all five are sending heartbeats every 30 seconds, and all are listed in the Eureka dashboard.

Now suppose there's a network issue and **instances 4 and 5** stop sending heartbeats. After 90 seconds of no renewal, Eureka evicts these two instances from the registry.

If another instance (say instance 3) also stops responding, Eureka will now have received heartbeats from only 2 out of 5 instances—just 40%. Since this is **below the 85% threshold**, Eureka **enters self-preservation mode**.

In this mode, **Eureka won’t remove any more instances**, even if they stop sending heartbeats. This is a conscious decision to **prevent further instability** caused by temporary network failures.

### Disabling Self-Preservation (Not Recommended)

If, for some reason, your architecture doesn't support this feature, it can be disabled using:

```properties
eureka.server.enable-self-preservation=false
```

⚠️ **Warning**: Disabling self-preservation is generally discouraged. During real-world network glitches, this could lead to your entire registry being wiped out, affecting service discovery and breaking client communication.

### Final Thoughts

The warning on the Eureka dashboard is simply a safety message. It means:

> “Heartbeats are below threshold, so I’m not expiring instances—just in case.”

So the next time you see this message, don’t panic. Let Eureka meditate 🧘 in peace.

Once the network stabilizes and instances start sending heartbeats again, the warning will disappear automatically.

---

## 14. Using Docker to Run Microservices

Ensure that docker is running. Insure that all microservices (specially `eurekaserver`) have `jib` related dependency:-

```xml
<plugin>
	<groupId>com.google.cloud.tools</groupId>
	<artifactId>jib-maven-plugin</artifactId>
	<version>3.4.5</version>
	<configuration>
		<to>
			<image>vikas9dev/${project.artifactId}:s8</image>
		</to>
	</configuration>
</plugin>
```
Update docker tag to "s8" in all the microservices. Delete the old docker images and generate the docker images for all microservice:-

```bash
mvn clean package jib:build -DskipTests
```
It have already logged in with DockerHub, then the above command will automatically push the docker images and won't store the images in the local. 

Update the docker compose files. In `docker-common.yml` file:-

```yaml
services:
  network-deploy-service:
    networks:
      - PeopleBank

  microservice-base-config:
    extends:
      service: network-deploy-service
    deploy:
      resources:
        limits:
          memory: 700m

  microservice-configserver-config:
    extends:
      service: microservice-base-config
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_PROFILES_ACTIVE: default
      SPRING_CONFIG_IMPORT: configserver:http://configserver:8071/

  microservice-eureka-config:
    extends:
      service: microservice-configserver-config
    depends_on:
      eurekaserver:
        condition: service_healthy
    environment:
      EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://eurekaserver:8070/eureka/
```

In `docker-compose.yml` file:-

```yaml
services:

    configserver:
        image: "vikas9dev/configserver:s8"
        container_name: configserver-ms
        ports:
            - "8071:8071"
        healthcheck:
            test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
            interval: 10s
            timeout: 5s
            retries: 10
            start_period: 10s
        extends:
            file: common-config.yml
            service: microservice-base-config

    eurekaserver:
        image: "vikas9dev/eurekaserver:s8"
        container_name: eurekaserver-ms
        ports:
            - "8070:8070"
        healthcheck:
            test: "curl --fail --silent localhost:8070/actuator/health/readiness | grep UP || exit 1"
            interval: 10s
            timeout: 5s
            retries: 10
            start_period: 10s
        extends:
            file: common-config.yml
            service: microservice-configserver-config
        environment:
            SPRING_APPLICATION_NAME: "eurekaserver"

    accounts:
        image: "vikas9dev/accounts:s8"
        container_name: accounts-ms
        ports:
            - "8080:8080"
        depends_on:
            configserver:
                condition: service_healthy
        environment:
            SPRING_APPLICATION_NAME: "accounts"
        extends:
            file: common-config.yml
            service: microservice-eureka-config

    loans:
        image: "vikas9dev/loans:s8"
        container_name: loans-ms
        ports:
            - "8090:8090"
        depends_on:
            configserver:
                condition: service_healthy
        environment:
            SPRING_APPLICATION_NAME: "loans"
        extends:
            file: common-config.yml
            service: microservice-eureka-config

    cards:
        image: "vikas9dev/cards:s8"
        container_name: cards-ms
        ports:
            - "9000:9000"
        depends_on:
            configserver:
                condition: service_healthy
        environment:
            SPRING_APPLICATION_NAME: "cards"
        extends:
            file: common-config.yml
            service: microservice-eureka-config

networks:
    PeopleBank:
        driver: bridge
```
Start the services using:-

```bash
docker compose up -d
```
Stop the services using:-

```bash
docker compose down
```

---

## 15. 🌀 Demonstrating Client-Side Load Balancing with Multiple Instances of the Loans Microservice

Until now, we’ve been testing our microservice setup with only a single instance of each service — particularly, just one instance of the **loans** and **cards** microservices. While this setup helps validate Eureka service registration, it doesn’t truly showcase the power of **client-side load balancing**. So in this section, we’ll scale our `loans` service horizontally by running two instances and observe how load is distributed between them.

### 🛠️ Step 1: Configure Multiple Instances in Docker Compose

To run multiple instances of the `loans` microservice, start by duplicating the `loans` service definition in your `docker-compose.yml` file. Ensure that:

- Each instance has a **unique `service name`** and **`container_name`**
- They expose **different ports** to avoid local port conflicts

Here’s a minimal snippet for the second instance:

```yaml
  loans1:
    image: "vikas9dev/loans:s8"
    container_name: loans-ms1
    ports:
      - "8091:8090"
    # other config
```

> 💡 Tip: Don't forget to update the service discovery configuration if needed, so Eureka can distinguish the instances.

### 🐳 Step 2: Spin Up the Services

Run the services using:

```bash
docker compose up -d
```

This might take a few minutes, especially as Docker consumes more CPU to run multiple containers. If your Docker engine is allocated a limited number of cores (e.g., 5), high CPU usage (~400% or more) is expected.

Once the containers are up, refresh the **Eureka Dashboard** (typically at `http://localhost:8070/eureka`) and you’ll see two instances of the `loans` service listed — something like this:

```
LOANS (2)
```

You can also confirm by accessing the raw registry:

```
http://localhost:8070/eureka/apps
```

Here, under `LOANS`, you should see both registered instances with different IPs/ports.

---

### 🔁 Step 3: Testing Client-Side Load Balancing

Let’s now test how client-side load balancing works in real-time. Here's the flow:

1. **Create Account Data** via the `createAccount` API.
2. **Create Card Data** using the corresponding endpoint.
3. **Create Loan Data** using **only one** of the loan instances (e.g., port `8090`).  
   > This simulates partial data availability across instances due to separate H2 databases.
4. **Invoke `fetchCustomerDetails` API** from the `accounts` service multiple times.

Because of the **OpenFeign Client** and **Eureka-based discovery**, each call to `fetchCustomerDetails` randomly routes to one of the `loans` instances.

#### 🧪 Observation

- **Request 1**: Routed to instance on port `8091` — loan not found → error returned.
- **Request 2**: Routed to instance on port `8090` — loan found → full customer details returned.
- **Repeat**: Alternates between successful and failed responses, confirming **round-robin** or similar client-side load balancing.

This experiment visually proves that your application is distributing load between available service instances using Eureka’s service registry and the Feign client.

> ⚠️ In production, where all instances share a centralized database like MySQL, this kind of data inconsistency won’t happen.

---

### 🎯 Conclusion

With this hands-on demo, you’ve successfully:

- Registered multiple instances of a microservice with Eureka
- Validated client-side load balancing using OpenFeign
- Witnessed real-time routing to different service instances

Later, when deploying to **Kubernetes**, we’ll also explore **server-side discovery and load balancing** patterns, typically managed by Kubernetes services and ingress controllers.

But for now — congrats on mastering client-side load balancing in a Spring Boot microservices setup! Take a breather and prepare for the next section. 🚀

---

Also see:- [GraalVM](https://www.youtube.com/watch?v=SISjh1krECE)