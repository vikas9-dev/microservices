# Making Microservice Resilient

## 1. Embracing the Challenge: Building Resilient Microservices

In this section, we‚Äôre diving into one of the most crucial aspects of building robust, production-ready microservices: **resiliency**.

### What Does ‚ÄúResilient‚Äù Mean?

Resiliency is the ability to withstand difficulties and bounce back stronger‚Äîjust like humanity did after facing a global challenge like COVID-19. Similarly, in the world of software, especially in microservices architecture, our services should be capable of surviving tough times‚Äîlike slow responses, network glitches, or partial failures‚Äîwithout crashing the entire system.

Microservices work together as a distributed system. When one of them slows down or fails, it shouldn‚Äôt bring down the whole application with it. That‚Äôs the essence of building **resilient microservices**.

### Key Challenges in Microservice Resiliency

Let‚Äôs explore some important questions we need to ask ourselves when designing resilient systems:

#### 1. **How Do We Prevent Cascading Failures?**

Imagine your client application sends a request that hits multiple services‚Äîsay, `accounts`, `loans`, and `cards`. If one of them (e.g., `cards`) is down or responding slowly, it can cause the `accounts` service to wait indefinitely. This waiting consumes system resources like threads and memory, which eventually causes other dependent services and even your gateway to get overloaded. This ripple effect is called a **cascading failure**, and it can take down your entire system.

![Cascading Failure](images/cascading-failure.png)

#### 2. **How Do We Handle Failures Gracefully?**

Instead of returning an error when one service fails, why not return **partial data**?

For example, if the `cards` service is unavailable, we should still return account and loan details to the client. This approach is called a **fallback mechanism**. It could mean returning cached data, default values, or even trying an alternative source. The goal? Never fail the entire request if you can still provide some useful data.

#### 3. **Can We Make Our Services Self-Healing?**

Temporary network hiccups or service slowdowns are common. In such cases, rather than waiting forever or giving up immediately, we can implement **timeouts** and **retries**.

For example:

- Retry a request 2‚Äì3 times before failing.
- Set a timeout so we don't keep waiting forever.
- Allow services a chance to recover from momentary issues without flooding the system with repeated requests.

By configuring these mechanisms properly, your services can recover on their own without human intervention‚Äîhence, **self-healing**.

### The Solution: Resilience Patterns with Resilience4j

To overcome these challenges, the Java ecosystem offers a solution: **Resilience4j**.

This lightweight fault-tolerance library was designed as a modern alternative to Netflix‚Äôs now-deprecated **Hystrix** library. Built with functional programming in mind (but still usable in traditional setups), Resilience4j provides a wide range of **resiliency patterns**, including:

- **Circuit Breaker**:- Used to stop making requests when a service invoked is failing.
- **Retry**:- Alternative paths to failing requests.
- **Fallback**:- Used to make retries when a service has temporarily failed.
- **Rate Limiter**:- Used to limit the number of calls that a service receives in a time.
- **Bulkhead**:- Used to limit the number of concurrent requests to a service to avoid overloading.
- **Time Limiter**
- **Cache**

These patterns help microservices handle failures gracefully and recover efficiently, without overwhelming your system.

Resilience4j also integrates seamlessly with popular frameworks like **Spring Boot** (2.x and 3.x), **Spring Cloud**, and **Micronaut**.

You can explore more about Resilience4j on its [official documentation site](https://resilience4j.readme.io/)‚Äîwhich I‚Äôll also include in the GitHub repo for quick reference.

---

## 2. Circuit Breaker Pattern in Microservices

**Circuit Breaker Pattern** is a powerful tool in our resiliency toolkit that helps prevent cascading failures in a microservices architecture.

Before we jump into how this works in software, let‚Äôs start with something familiar: **the electrical circuit breaker** we all use at home or in the office.

### üè† What Does a Circuit Breaker Do?

In an electrical system, a **circuit breaker** is a safety device that protects the wiring and appliances from overload or short circuits. If it detects a fault‚Äîlike excessive current‚Äîit **‚Äútrips‚Äù** or **opens the circuit** to stop the flow of electricity. This prevents potential damage or fire hazards.

![Circuit Breaker](images/circuit-breaker.png)

Think about it: if a bulb is connected to a faulty circuit, and there‚Äôs no breaker in place, the surge could destroy it. But with a breaker, the faulty circuit is immediately disconnected, keeping the bulb safe.

### ‚öôÔ∏è How Does This Relate to Microservices?

Now, bring this same idea into the world of software.

In a microservices ecosystem, services often rely on each other via remote calls. But what happens when one of those services becomes slow or unresponsive? Maybe due to:

- A spike in traffic
- Network latency
- Temporary downtime
- Resource exhaustion

These issues are usually **transient**‚Äîthey recover after a short time. But during that period, if all requests keep flowing toward the unhealthy service, it creates a ripple effect, dragging down the entire system.

That‚Äôs where the **Circuit Breaker Pattern** comes in.

### üõë What Does a Circuit Breaker Do in Software?

Just like in electrical systems, a **software circuit breaker** monitors traffic‚Äîin this case, **remote calls to a service** (like our Cards microservice).

If it detects a high failure rate or slow responses over a configured threshold, it **‚Äútrips‚Äù**‚Äîblocking further calls to the faulty service temporarily.

Instead of letting other microservices (like Accounts or the API Gateway) wait unnecessarily for a slow response, the circuit breaker **fails fast**. This means the failure is returned immediately, preventing resources like threads, memory, and CPU from being consumed unnecessarily.

This approach ensures the failure in one service doesn‚Äôt cascade and take down others‚Äî**a classic ripple effect is avoided**.

### üîÑ How Does Recovery Work?

A circuit breaker doesn‚Äôt stay open forever.

After a cooling-off period (say 30 or 90 seconds), it allows **some trial requests** to pass through to see if the service has recovered. If those calls succeed, the breaker **closes** again, allowing traffic to resume normally.

If the service is still unhealthy, the breaker stays open a little longer, checking again periodically.

### ‚úÖ Key Benefits of the Circuit Breaker Pattern

1. **Fail Fast:** No more waiting 10+ seconds for a response that may never come. Fail the call instantly and move on.
2. **Graceful Degradation:** Combine the breaker with a **fallback** strategy‚Äîlike cached data or default values‚Äîto still serve partial responses.
3. **Self-Healing Architecture:** Give your service time to recover by reducing pressure. Once it‚Äôs healthy, traffic resumes automatically.

### How Circuit Breaker Pattern Controls Traffic to a Microservice

By default, the circuit breaker doesn‚Äôt monitor all your microservices ‚Äî you need to **explicitly configure** it for each one where you want fault tolerance. Once configured, it manages traffic using **three key states**: **Closed**, **Open**, and **Half-Open**.

![Circuit Breaker Pattern](images/circuit-breaker-pattern.png)

#### 1. Closed State

When the application starts, the circuit breaker is initially in the **closed** state. This means it allows **all requests** to flow through to the microservice.

Just like a closed electrical circuit lets current pass through, the closed state here allows full traffic. During this phase, the circuit breaker **monitors the success and failure rate** of the requests. If it notices a significant number of failures‚Äîsay, more than a configured threshold like **50%**‚Äîit transitions to the **open** state.

#### 2. Open State

Once the circuit breaker is **open**, it **blocks all incoming traffic** to the microservice. Any request is immediately failed, and a fallback or error response is returned to the caller.

This prevents cascading failures and ripple effects across other services that depend on the failing one. The open state acts as a **cooling period**, giving the troubled microservice time to recover.

But the open state isn‚Äôt permanent. After a configured delay (e.g., **90 seconds**), the circuit breaker transitions to a **half-open** state to test if recovery is possible.

#### 3. Half-Open State

In the **half-open** state, the circuit breaker allows only a **small number of requests** (e.g., 10 or 20) to pass through as a test.

- If **more than 50%** of those requests still fail, the circuit breaker reverts back to the open state and waits another 90 seconds.
- If the **majority succeed**, it assumes the microservice is healthy again and transitions back to the **closed** state, fully restoring traffic flow.

This cycle repeats until the service is stable.

#### Easy to Implement with Resilience4j and Spring Boot

Although it might sound complex, implementing this pattern is surprisingly easy using libraries like **Resilience4j** along with **Spring Boot**. These tools make it simple to configure thresholds, timeouts, and fallback behaviors for your microservices.

---

## 3. Implementing the Circuit Breaker Pattern in Microservices

### Step 1: Add Resilience4j to the Gateway Server

We‚Äôll begin implementing the circuit breaker pattern in the **Gateway Server**, which acts as the edge server for our microservices. This is a common practice in real-world applications, as it provides a single point to manage service reliability.

Open the `pom.xml` of the Gateway Server and add the following dependency:

```xml
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-starter-circuitbreaker-reactor-resilience4j</artifactId>
</dependency>
```

This dependency allows us to use circuit breaker filters with the Spring Cloud Gateway, which follows the reactive model. Reload Maven to sync the changes.

### Step 2: Configure Circuit Breaker in Gateway Routes

Open the Gateway Server‚Äôs main Spring Boot class where routing is configured. We'll add a new circuit breaker filter for the `/accounts` path:

```java
.route("account-service", r -> r.path("/peoplebank/accounts/**")
  .filters(f -> f
    .rewritePath("/peoplebank/accounts/(?<segment>.*)", "/${segment}")
    .circuitBreaker(config -> config
      .setName("accountCircuitBreaker")
    )
  )
  .uri("lb://ACCOUNTS"))
```

Here, `"accountCircuitBreaker"` is the name we assign to this circuit breaker instance.

### Step 3: Add Circuit Breaker Configuration to `application.yml`

Now, we‚Äôll configure circuit breaker properties inside `application.yml`:

```yaml
resilience4j.circuitbreaker:
  configs:
    default:
      slidingWindowSize: 10
      permittedNumberOfCallsInHalfOpenState: 2
      failureRateThreshold: 50
      waitDurationInOpenState: 10000
```

These properties control the behavior of the circuit breaker:

- **`slidingWindowSize: 10`**: Monitors the last 10 calls before deciding on state change.
- **`permittedNumberOfCallsInHalfOpenState: 2`**: Allows 2 test calls during half-open state.
- **`failureRateThreshold: 50`**: Circuit will open if 50% of calls fail.
- **`waitDurationInOpenState: 10000`**: Stays in open state for 10 seconds before trying half-open.

You can customize these settings per circuit breaker by replacing `default` with the name, e.g., `accountCircuitBreaker`.

### Step 4: Start the Microservices

Start the microservices in this order:

1. **Config Server**
2. **Eureka Server**
3. **Accounts Microservice**
4. **Gateway Server**

(We‚Äôll keep `cards` and `loans` services off for now, focusing only on `accounts`.)

Once the services are running, validate them:

- Check **Eureka Dashboard** to confirm registration.
- Open `http://localhost:8072/actuator/circuitbreakers` to view circuit breaker statuses.

At this point, the status for `accountCircuitBreaker` should be `CLOSED`.

```json
{
  "circuitBreakers": {}
}
```

### Step 5: Trigger the Circuit Breaker

In Postman, call:

```
GET http://localhost:8072/easybank/accounts/api/contact-info
```

This returns the contact information successfully. Now, if we try to access the `http://localhost:8072/actuator/circuitbreakers` endpoint again, we‚Äôll see the following response:

```json
{
  "circuitBreakers": {
    "accountCircuitBreaker": {
      "failureRate": "-1.0%",
      "slowCallRate": "-1.0%",
      "failureRateThreshold": "50.0%",
      "slowCallRateThreshold": "100.0%",
      "bufferedCalls": 1,
      "failedCalls": 0,
      "slowCalls": 0,
      "slowFailedCalls": 0,
      "notPermittedCalls": 0,
      "state": "CLOSED"
    }
  }
}
```

To simulate failure, set a breakpoint inside the `contactInfo()` method of the `AccountsController` and don‚Äôt release it. This creates a delayed response, leading to timeouts (504 Gateway Timeout).

After several failed attempts, the circuit breaker will transition:

- From `CLOSED` ‚Üí `OPEN` once the failure threshold is breached.
- From `OPEN` ‚Üí `HALF_OPEN` after `waitDurationInOpenState`.
- Based on test calls during half-open, it will either go back to `OPEN` or reset to `CLOSED`.

You can monitor this behavior using:

- `http://localhost:8072/actuator/circuitbreakers`
- `http://localhost:8072/actuator/circuitbreakerevents?name=accountCircuitBreaker`

In closed state, the `GET http://localhost:8072/easybank/accounts/api/contact-info` returns 503 (Service Unavailable).

### Step 6: Reset the Circuit Breaker

Remove the breakpoint and start sending successful requests again. Eventually, the circuit breaker will transition:

- From `HALF_OPEN` ‚Üí `CLOSED`, once the calls succeed.

This confirms that the circuit breaker is now healthy and allows traffic again.

With this setup, you‚Äôve successfully implemented a circuit breaker at the Gateway Server. It helps protect your system by failing fast and conserving resources when a downstream service is struggling or unresponsive.

In the next section, we‚Äôll explore how to apply the circuit breaker pattern **within individual microservices** like the `accounts` service itself. Stay tuned!

---

## 4. Implementing a Fallback Mechanism for the Circuit Breaker in the Gateway Server

Up to this point, we have successfully implemented a **Circuit Breaker pattern** within our Gateway server. However, it currently lacks a **fallback mechanism**.

Without a fallback, whenever a service is down or unresponsive, the client receives low-level runtime exception details such as `Service Unavailable` or `Gateway Timeout`. In real-world applications, this is not a user-friendly or acceptable approach. Clients, especially UI applications, should not be exposed to such technical exceptions. Instead, they should receive a meaningful, business-level response that makes sense in the context of your application.

### Why We Need a Fallback

A **fallback mechanism** allows us to handle failures gracefully. When a dependent service fails, we can return a predefined response, log the error, notify support teams, or even provide cached/default data. This enhances user experience and system resilience.

### Creating a Fallback Controller

To implement a fallback in our **Gateway server** (built using **Spring Cloud Gateway** and **Spring WebFlux**), we start by creating a new controller class. Inside the `gateway` module:

1. **Create a new package** named `controller`.
2. Inside this package, create a class named `FallbackController`.
3. Annotate the class with `@RestController` to expose REST APIs.

Here‚Äôs a simple fallback endpoint:

```java
@RestController
@RequestMapping("/fallback")
public class FallbackController {
    @GetMapping("/contactSupport")
    public Mono<String> contactSupport() {
        return Mono.just("An error occurred. Please try again later or contact the support team.");
    }
}
```

This endpoint returns a business-friendly message wrapped in a `Mono<String>` (as required by Spring WebFlux).

### Integrating the Fallback with the Circuit Breaker

Now that we have a fallback API, the next step is to **wire it into the circuit breaker configuration** in our Gateway server.

In the route configuration (likely inside `application.yml` or Java config), you‚Äôll find the circuit breaker setup. Add the fallback URI using the `setFallbackUri()` method:

```java
.route("accounts_route", r -> r
	.path("/peoplebank/accounts/**")
	.filters(f -> f
		.rewritePath("/peoplebank/accounts/(?<segment>.*)", "/${segment}")
	    .addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
		.circuitBreaker(config -> config
			.setName("accountCircuitBreaker")
		    .setFallbackUri("forward:/contactSupport")
		)
	)
	.uri("lb://ACCOUNTS")
)
```

This tells the Gateway: _if the downstream service call fails, forward the request to the fallback controller‚Äôs `/contactSupport` endpoint_.

### Testing the Fallback

To test the fallback:

1. Restart the Gateway server and ensure all services are running.
2. Use Postman or curl to send a request to a valid route. You should receive a normal, happy response.
3. Simulate a failure by stopping the downstream service or introducing a delay.
4. Send the request again. This time, you should see the fallback message:
   > _"An error occurred. Please try again later or contact the support team."_

This confirms the fallback is working as expected. When the downstream service is available again, the fallback will not be triggered.

---

## 5.

---

## 6.

---

## 7.

---

## 8.

---

## 9.
